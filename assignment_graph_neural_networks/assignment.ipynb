{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment â€” Graph Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment lines in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl.data import CoraGraphDataset\n",
    "from dgl import function as fn\n",
    "from dgl.nn import GATConv, SAGEConv\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CoraGraphDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch quick start\n",
    "\n",
    "Here are the basics of PyTourch. If you are familiar with this material, skip it. \n",
    "\n",
    "First, let us generate a toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "X = np.linspace(-10, 10, N)\n",
    "np.random.seed(0)\n",
    "y = np.sin(X) + np.random.normal(0, 0.2, N)\n",
    "X = minmax_scale(X, (-1, 1))\n",
    "y = minmax_scale(y, (-1, 1))\n",
    "plt.scatter(X, y, s=3)\n",
    "plt.ylim(-1.5, 1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert np.arrays into tensors via `torch.FloatTensor` (float type) or `torch.LongTensor` (int type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.FloatTensor(X[:, None])\n",
    "y_tensor = torch.FloatTensor(y[:, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple shallow perceptron with layer sizes $1 \\to 64 \\to 64 \\to 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(1, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = nn.MSELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train loop constists of: computing loss, grad, making a step to the optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 300\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    y_pred = model.forward(X_tensor)\n",
    "    loss = MSE(y_pred, y_tensor)\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    plt.scatter(X, y, s=3)\n",
    "    plt.plot(X, y_pred.detach().numpy(), linewidth=2, c='tab:orange')\n",
    "    plt.title('Epoch: {}/{}, Loss: {:.4f}'.format(i+1, n_epochs, loss.item()))\n",
    "    plt.ylim(-1.5, 1.5)\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Spectral graph convolutional network (1 point)\n",
    "\n",
    "Graph convolution mathematically is defined as follows:\n",
    "\n",
    "$$h_i^{(l+1)} = \\sigma(b^{(l)} + \\sum_{j\\in\\mathcal{N}(i)}\\frac{1}{c_{ij}}h_j^{(l)}W^{(l)})$$\n",
    "\n",
    "where $\\mathcal{N}(i)$ is the set of neighbors of node $i$,$c_{ij}$ is the product of the square root of node degrees (i.e.,  $c_{ij} = \\sqrt{|\\mathcal{N}(i)|}\\sqrt{|\\mathcal{N}(j)|}$), and $\\sigma$ is an activation function.\n",
    "\n",
    "Build a cora dataset again, but without conversion into numpy or networkx. These are tensors by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CoraGraphDataset()\n",
    "G = data[0]\n",
    "features = G.ndata['feat']\n",
    "labels = G.ndata['label']\n",
    "train_mask = G.ndata['train_mask']\n",
    "test_mask = G.ndata['test_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an one-hot encoded feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And labels of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero in-degree nodes will lead to invalid output value. This is because no message will be passed to those nodes, the aggregation function will be appied on empty input. A common practice to avoid this is to add a self-loop for each node in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = dgl.add_self_loop(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GCNLayer` should work as follows:\n",
    "\n",
    "1. Apply dropout to the input hidden state if it is not equal to `0` or `None` and if we are in train mode\n",
    "2. Apply linear layer to the hidden state\n",
    "3. Multiply result of point 2 by normalization (`self.norm`)\n",
    "4. Aggregate data over neighbourhood multiply adjacency matrix and hidden state)\n",
    "5. Multiply result of point 4 by normalization (`self.norm`)\n",
    "6. Return hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55480f9c98f9765f980aebb3f03b22db",
     "grade": false,
     "grade_id": "cell-49c0a67459fef8d6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 g: nx.Graph,\n",
    "                 in_feats: int,\n",
    "                 out_feats: int,\n",
    "                 dropout: float = None):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.adj = torch.Tensor(nx.to_numpy_array(g))\n",
    "        self.norm = np.power(self.adj.sum(0), -0.5)\n",
    "        self.norm[np.isinf(self.norm)] = 0\n",
    "        self.norm = torch.Tensor(self.norm).unsqueeze(1)\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "    def forward(self, h):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "275f25b930fb81bd0cc975bdef9cf47c",
     "grade": true,
     "grade_id": "cell-3773092e16e3398b",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "layer = GCNLayer(G.to_networkx(), 100, 10, 0.5)\n",
    "assert str(layer) == 'GCNLayer(\\n  (linear): Linear(in_features=100, out_features=10, bias=True)\\n  (dropout): Dropout(p=0.5, inplace=False)\\n)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete a class `GCN`. Use `GCNLayer` layers with sizes $1433 \\to 16 \\to 7$, dropout for first layer is equal to zero, for the second layer you need to pass argument. Use `F.relu` activation after the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3da8c02a0f2c2381226cea954abb6616",
     "grade": false,
     "grade_id": "cell-5e0c138f5ccaea0e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, g, dropout=None, bias=False):\n",
    "        super(GCN, self).__init__()\n",
    "        self.g = g\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, features):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0eb2a752a5775e5845c41b9a598b10f8",
     "grade": true,
     "grade_id": "cell-fe037acfca3092bc",
     "locked": true,
     "points": 0.3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = GCN(G.to_networkx(), dropout=0.1)\n",
    "assert str(model) == 'GCN(\\n  (conv1): GCNLayer(\\n    (linear): Linear(in_features=1433, out_features=16, bias=True)\\n  )\\n  (conv2): GCNLayer(\\n    (linear): Linear(in_features=16, out_features=7, bias=True)\\n    (dropout): Dropout(p=0.1, inplace=False)\\n  )\\n)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification tasks we will use `CrossEntropy` loss. Change an optimizer if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CrossEntropy = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the train loop. To speed up calculation test loss, use `torch.no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a8d44c28282242c03c29e880342c765",
     "grade": false,
     "grade_id": "cell-859836ad344792e3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "\n",
    "log = []\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    #train_loss = <YOUR CODE>\n",
    "    #test_loss = <YOUR CODE>\n",
    "    \n",
    "    log.append([train_loss, test_loss])\n",
    "    \n",
    "    plt.plot(np.array(log))\n",
    "    plt.title('Epoch: {}/{}'.format(i+1, n_epochs))\n",
    "    plt.legend(['train', 'test'])\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "988ee7e3de830a615ddb26499566485b",
     "grade": true,
     "grade_id": "cell-0dbd2da971a339fe",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "logits = model.forward(features)\n",
    "y_pred = torch.argmax(logits[test_mask], 1)\n",
    "score = balanced_accuracy_score(labels[test_mask], y_pred)\n",
    "assert score > 0.75\n",
    "print('Balanced accuracy: {:.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Graph autoencoder (2 points)\n",
    "\n",
    "In the previous task we have trained our model with supervised loss (node classification task).\n",
    "\n",
    "One can train GNN in unsupervised fashion. To do so we can state our problem as a graph autoencoder. We will train embeddings in the way to reconstruct adjacency matrix.\n",
    "\n",
    "We will decode our adjacency matrix with `InnerProductDecoder`.\n",
    "\n",
    "You need to implement `forward` function. It works as follows:\n",
    "\n",
    "1. Apply dropout layer if dropout rate > 0 and not None and we are in train mode.\n",
    "2. Multiply embeddings matrix and transposed embedding matrix\n",
    "3. Apply activation if it is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e41e8f0e3a45d36ed59274a66c229085",
     "grade": false,
     "grade_id": "cell-31af15ac93053cc6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class InnerProductDecoder(nn.Module):\n",
    "    def __init__(self, activation=torch.sigmoid, dropout=0.1):\n",
    "        super(InnerProductDecoder, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, h):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f049476539ac5d2f0441973a8901eb14",
     "grade": true,
     "grade_id": "cell-5bd597fc40f459f5",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "decoder = InnerProductDecoder(torch.sigmoid, dropout=None)\n",
    "adj = decoder(torch.Tensor(np.arange(10).reshape(5, 2)))\n",
    "assert round(float(adj.numpy()[0, 0]), 4) == 0.7311"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ReconstructionLoss` function takes dgl.Graph as class parameters. After initialization it should extract adjacency from graph as a dense tensor, and calculate `pos_weight` as $$(N * N - E) / E$$, where $N$ is a number of nodes and $E$ is a number of edges.\n",
    "\n",
    "Implement `__call__` function that calculates `F.binary_cross_entropy_with_logits` between predicted_adjacency and real adjacency of a graph with `pos_weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4983e4aeb2e2ca974d9deb827250bda",
     "grade": false,
     "grade_id": "cell-f21ed6c1f82b2b7b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class ReconstructionLoss:\n",
    "    def __init__(self, g):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def __call__(self, predicted_adjacency):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "503998ae810b38777802f99973b5a1d0",
     "grade": true,
     "grade_id": "cell-dc7a5b14910f1b1b",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "rec_loss = ReconstructionLoss(G)\n",
    "assert round(rec_loss(G.adjacency_matrix().to_dense()).item(), 4) == 1.0046"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the train loop so that node embeddings obtained from the GCN model are fed to the decoder, reconstruction loss of a predicted adjacency matrix is computed and then an optimization step is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2f579090c5e7e714b62c5cc85a1d511",
     "grade": false,
     "grade_id": "cell-ab892da4f2b04e92",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "\n",
    "log = []\n",
    "\n",
    "model = nn.Sequential(\n",
    "    GCNLayer(G.to_networkx(), 1433, 16, 0.1),\n",
    "    nn.ReLU(),\n",
    "    GCNLayer(G.to_networkx(), 16, 16, 0.5),\n",
    ")\n",
    "decoder = InnerProductDecoder()\n",
    "rec_loss = ReconstructionLoss(G)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    #train_loss = <YOUR CODE>\n",
    "    \n",
    "    log.append([train_loss])\n",
    "    \n",
    "    plt.plot(np.array(log))\n",
    "    plt.title('Epoch: {}/{}'.format(i+1, n_epochs))\n",
    "    plt.legend(['train'])\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will validate how unsupervised embeddings will work for node classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb2934592b76869b0dff13212183ca86",
     "grade": true,
     "grade_id": "cell-08af8fff077a7121",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model.eval()\n",
    "emb = model.forward(features).data.numpy()\n",
    "labs = labels.numpy()\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(emb[train_mask], labs[train_mask])\n",
    "\n",
    "score = balanced_accuracy_score(labels[test_mask], torch.Tensor(lr.predict(emb[test_mask])))\n",
    "assert score >= 0.45\n",
    "print('Balanced accuracy: {:.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Graph attention network (3 point)\n",
    "\n",
    "Let us apply Graph Attention Network over an input signal.\n",
    "\n",
    "$$h_i^{(l+1)} = \\sum_{j\\in \\mathcal{N}(i)} \\alpha_{i,j} W^{(l)} h_j^{(l)}$$\n",
    "\n",
    "where $\\alpha_{ij}$ is the attention score bewteen node $i$ and node $j$:\n",
    "\n",
    "$$\\alpha_{ij}^{l} = \\text{Softmax}_{i} (e_{ij}^{l})$$\n",
    "\n",
    "$$e_{ij}^{l} = \\text{LeakyReLU}\\left(\\vec{a}^T [W h_{i} \\| W h_{j}]\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CoraGraphDataset()\n",
    "G = data[0]\n",
    "G = dgl.add_self_loop(G)\n",
    "features = G.ndata['feat']\n",
    "labels = G.ndata['label']\n",
    "train_mask = G.ndata['train_mask']\n",
    "test_mask = G.ndata['test_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete a class `GAT`. Use `GATConv` layers:\n",
    "`GATConv(1433, 8, num_heads=8)` $\\to$ `GATConv(8*8, 7, num_heads=1)`\n",
    "\n",
    "In `forward` use \n",
    "* `tensor.view(-1, m * n)` to reshape `(n_features, n_heads, out_dim) -> (n_features, n_heads * out_dim)`\n",
    "* `F.elu` activation after reshape\n",
    "* `tensor.squeeze` to decrease shape `(n_features, 1, out_dim) -> (n_features, out_dim)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cdc8345eb40be185359aeb01fff87fde",
     "grade": false,
     "grade_id": "cell-2ac8340166410ded",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()\n",
    "        #self.conv1 = <YOUR CODE>\n",
    "        #self.conv2 = <YOUR CODE>\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, G, features):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f46ec48603ff02a5aedabf02176194a",
     "grade": true,
     "grade_id": "cell-8b1838d547caf32d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = GAT()\n",
    "assert str(model) == 'GAT(\\n  (conv1): GATConv(\\n    (fc): Linear(in_features=1433, out_features=64, bias=False)\\n    (feat_drop): Dropout(p=0.0, inplace=False)\\n    (attn_drop): Dropout(p=0.0, inplace=False)\\n    (leaky_relu): LeakyReLU(negative_slope=0.2)\\n  )\\n  (conv2): GATConv(\\n    (fc): Linear(in_features=64, out_features=7, bias=False)\\n    (feat_drop): Dropout(p=0.0, inplace=False)\\n    (attn_drop): Dropout(p=0.0, inplace=False)\\n    (leaky_relu): LeakyReLU(negative_slope=0.2)\\n  )\\n)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CrossEntropy = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), \n",
    "                       lr=0.005, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the train loop. To speed up calculation test loss, use `torch.no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d409df1ea0e998208b32f57cfb9b814",
     "grade": false,
     "grade_id": "cell-8d019b9371d11f59",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 150\n",
    "\n",
    "log = []\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    #train_loss = <YOUR CODE>\n",
    "    #test_loss = <YOUR CODE>\n",
    "    \n",
    "    log.append([train_loss, test_loss])\n",
    "    \n",
    "    plt.plot(np.array(log))\n",
    "    plt.title('Epoch: {}/{}'.format(i+1, n_epochs))\n",
    "    plt.legend(['train', 'test'])\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3b5d5976d418d83fb7ca869601a8f74",
     "grade": true,
     "grade_id": "cell-f0b5cbfec9b4510d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "logits = model.forward(G, features)\n",
    "y_pred = torch.argmax(logits[test_mask], 1)\n",
    "score = balanced_accuracy_score(labels[test_mask], y_pred)\n",
    "assert score > 0.7\n",
    "print('Balanced accuracy: {:.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4. GraphSAGE (4 points)\n",
    "\n",
    "Consider GraphSAGE, a representation learning technique suitable for dynamic graphs. GraphSAGE is capable of predicting embedding of a new node, without requiring a re-training procedure. To do so, GraphSAGE learns aggregator functions that can induce the embedding of a new node given its features and neighborhood. This is called inductive learning.\n",
    "\n",
    "$$h_{\\mathcal{N}(i)}^{(l+1)} = \\mathrm{aggregate}\\left(\\{h_{j}^{l}, \\forall j \\in \\mathcal{N}(i) \\}\\right)$$\n",
    "$$h_{i}^{(l+1)} = \\sigma \\left(W \\cdot \\text{concat}(h_{i}^{l}, h_{\\mathcal{N}(i)}^{l+1}) \\right)$$\n",
    "$$h_{i}^{(l+1)} = \\mathrm{norm}(h_{i}^{l})$$\n",
    "\n",
    "Aggregator types here can be `mean`, `gcn`, `pool`, `lstm`. Consider GraphSAGE on the Karate Club graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.karate_club_graph()\n",
    "\n",
    "labels = [1 if i=='Mr. Hi' else 0 for i in nx.get_node_attributes(G, 'club').values()]\n",
    "labels = torch.LongTensor(labels)\n",
    "features = torch.FloatTensor(np.arange(0, 34)[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_kamada_kawai(\n",
    "    G, with_labels=True, \n",
    "    node_color=['tab:orange' if i==1 else 'tab:blue' for i in labels], \n",
    "    cmap=plt.cm.tab10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us delete a node 31, train a model and then return it and predict its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(34)\n",
    "idx = idx[idx != 31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New labels and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels[idx]\n",
    "features = features[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_kamada_kawai(\n",
    "    G.subgraph(idx), with_labels=True, \n",
    "    node_color=['tab:orange' if i==1 else 'tab:blue' for i in labels], \n",
    "    cmap=plt.cm.tab10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let choose test and train nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = [31, 32, 0, 11, 13, 2, 23, 29, 8]\n",
    "train_idx = list(set(np.arange(33)).difference(test_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the graph, test nodes are gray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_color = np.ones((33, 3))\n",
    "node_color[labels == 0] = plt.cm.tab10(0)[:3]\n",
    "node_color[labels == 1] = plt.cm.tab10(1)[:3]\n",
    "node_color[test_idx] = (0.9, 0.9, 0.9)\n",
    "\n",
    "nx.draw_kamada_kawai(G.subgraph(idx), with_labels=True, \n",
    "                     node_color=node_color, cmap=plt.cm.tab10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a dgl graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_graph = dgl.from_networkx(G.subgraph(idx))\n",
    "initial_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete a class `SAGE`. Use `SAGEConv` layers with sizes $1 \\to 16 \\to 2$ and `mean` aggregation function. Put `F.relu` activation after the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9399531298f5c0c2a65de6d6a0a15776",
     "grade": false,
     "grade_id": "cell-c83ca7ab5833bc2a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.conv1 = <YOUR CODE>\n",
    "        #self.conv2 = <YOUR CODE>\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, graph, features):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b64d5415e2e58b1bcd0641c27cc528d5",
     "grade": true,
     "grade_id": "cell-a7513e546c544396",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = SAGE()\n",
    "assert str(model) == 'SAGE(\\n  (conv1): SAGEConv(\\n    (feat_drop): Dropout(p=0.0, inplace=False)\\n    (fc_self): Linear(in_features=1, out_features=16, bias=True)\\n    (fc_neigh): Linear(in_features=1, out_features=16, bias=True)\\n  )\\n  (conv2): SAGEConv(\\n    (feat_drop): Dropout(p=0.0, inplace=False)\\n    (fc_self): Linear(in_features=16, out_features=2, bias=True)\\n    (fc_neigh): Linear(in_features=16, out_features=2, bias=True)\\n  )\\n)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CrossEntropy = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 150\n",
    "\n",
    "log = []\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    logits = model.forward(initial_graph, features)\n",
    "    loss = CrossEntropy(logits[train_idx], labels[train_idx])\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model.forward(initial_graph, features)\n",
    "        test_loss = CrossEntropy(logits[test_idx], labels[test_idx])\n",
    "    \n",
    "    log.append([loss.item(), test_loss.item()])\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(np.array(log))\n",
    "    plt.title('Epoch: {}/{}'.format(i+1, n_epochs))\n",
    "    plt.legend(['train', 'test'])\n",
    "    \n",
    "    y_pred = torch.argmax(logits, 1)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    nx.draw_kamada_kawai(\n",
    "        G.subgraph(idx), with_labels=True, \n",
    "        node_color=['tab:orange' if i==1 else 'tab:blue' for i in y_pred], \n",
    "        cmap=plt.cm.tab10)\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "    clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check that prediction for the node 31 is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3dab650bfd86a91be05e721cd00e089e",
     "grade": true,
     "grade_id": "cell-41e44fe2c4bf18a1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "graph = dgl.from_networkx(G)\n",
    "labels = [1 if i=='Mr. Hi' else 0 for i in nx.get_node_attributes(G, 'club').values()]\n",
    "labels = torch.LongTensor(labels)\n",
    "features = torch.FloatTensor(np.arange(0, 34)[:, None])\n",
    "predictions = torch.argmax(model(graph, features), 1)\n",
    "assert predictions[31] == labels[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
