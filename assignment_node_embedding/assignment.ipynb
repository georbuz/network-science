{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment â€” Node Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U gensim==3.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import requests\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORA Exploratory Data Analysis\n",
    "\n",
    "The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Let us take a closer look at this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/netspractice/network-science/main/datasets/cora_cites.txt'\n",
    "open('cora_cites.txt', 'wb').write(requests.get(url).content)\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/netspractice/network-science/main/datasets/cora_content.txt'\n",
    "open('cora_content.txt', 'wb').write(requests.get(url).content);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of nodes in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora = nx.read_edgelist('cora_cites.txt')\n",
    "len(cora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of nodes in a gigantic connected component (GCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcc_nodes = sorted(list(nx.connected_components(cora)), \n",
    "                   key=lambda x: len(x))[-1]\n",
    "gcc_cora = cora.subgraph(gcc_nodes).copy()\n",
    "len(gcc_cora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nodes content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_content = pd.read_csv('cora_content.txt', sep='\t', \n",
    "                           header=None, index_col=0)\n",
    "cora_content.index = cora_content.index.astype('str')\n",
    "cora_content.index.name = 'node'\n",
    "cora_content.iloc[:5, :20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words. In this assignment, we will only work with categories and will not touch any information about words.\n",
    "\n",
    "Examples of node categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = cora_content.loc[gcc_nodes, [1434]]\n",
    "category = category.rename(columns={1434: 'category_name'})\n",
    "category.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Category distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(*np.unique(category, return_counts=True));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename categories to integer numbers (ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category['category_id'] = np.unique(category.category_name, \n",
    "                                    return_inverse = True)[1]\n",
    "category.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assortativity coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(gcc_cora, category.category_id.to_dict(), 'category')\n",
    "gcc_cora = nx.convert_node_labels_to_integers(gcc_cora)\n",
    "round(nx.attribute_assortativity_coefficient(gcc_cora, 'category'), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Node embedding visualization (1.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare embedding algorithms that we saw in previous assignments: \n",
    "* Laplacian Eigenmaps\n",
    "* Truncated SVD of an adjacency matrix\n",
    "* DeepWalk\n",
    "* Walklets\n",
    "\n",
    "There is a usefull python package [*Karate Club*](https://github.com/benedekrozemberczki/karateclub) that contains implementations of these algorithms. Also we will use sklearn implementation of truncated SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install karateclub==1.0.24 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karateclub import DeepWalk, Walklets, LaplacianEigenmaps\n",
    "from sklearn.decomposition import TruncatedSVD, PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding algorithms in Karate Club have a general interface\n",
    "\n",
    "```\n",
    "model.fit(graph)\n",
    "embedding = model.get_embedding()\n",
    "```\n",
    "\n",
    "However, the one inconvenient thing is that Walklets make a concatenation of Word2Vec embeddings to obtain final embeddings. In this way, the output dimensionality will be Word2Vec dimensionality multiplied by window size. A piece of source code:\n",
    "\n",
    "```python\n",
    "def get_embedding(self) -> np.array:\n",
    "    r\"\"\"Getting the node embedding.\n",
    "\n",
    "    Return types:\n",
    "        * **embedding** *(Numpy array)* - The embedding of nodes.\n",
    "    \"\"\"\n",
    "    return np.concatenate(self._embedding, axis=1)\n",
    "```\n",
    "\n",
    "Sometimes, it is usefull to have deep representation of nodes, but  we want to compare embeddings with the same dimensionality, so let us define our own class `PCAWalklets` that inherits `Walklets` and performs PCA reduction of embeddings into `self.dimensions` in the `get_embedding` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c723079eb903af6b2b56ad5dada91ef",
     "grade": false,
     "grade_id": "cell-1be4879cc8190e43",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class PCAWalklets(Walklets):\n",
    "    def get_embedding(self):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "799029b3b6f99a2b503bc327ae1e7f8f",
     "grade": true,
     "grade_id": "cell-7c5dd45231ab30af",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_model = PCAWalklets()\n",
    "test_model.fit(nx.karate_club_graph())\n",
    "test_emb = test_model.get_embedding()\n",
    "assert test_emb.shape == (34, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualize the 2d embeddings.\n",
    "\n",
    "Write a function `xy_embeddings` that takes a graph, compute 16d embeddings, reduce them into 2d via PCA and returns in the order:\n",
    "* Laplacian Eigenmaps\n",
    "* Truncated SVD of an adjacency matrix\n",
    "* DeepWalk\n",
    "* Walklets\n",
    "\n",
    "*Hints:*\n",
    "* *Suggested hyperparameters for DeepWalk and Walklets are `walk_number=10`, `walk_length=30`, `window_size=10`*\n",
    "* *You do not need reduce Walklets embeddgins to 2d, just directly use `PCAWalklets` with dimensionality 2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a3381cad3ce581fde97f70776fac00b",
     "grade": false,
     "grade_id": "cell-d33f4faf41b0da0e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def xy_embeddings(graph):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f48e1ae71e8a43afbbb89f0b4c46ae2",
     "grade": true,
     "grade_id": "cell-ce2531dac614957f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "laplacian_emb, svd_emb, deep_walk_emb, walklets_emb = xy_embeddings(gcc_cora)\n",
    "assert (deep_walk_emb.shape == laplacian_emb.shape \n",
    "        == svd_emb.shape == walklets_emb.shape == (2485, 2))\n",
    "assert -0.5 < laplacian_emb.min() < laplacian_emb.max() < 0.5\n",
    "assert svd_emb[0].sum() > 15\n",
    "assert round(abs(np.corrcoef(deep_walk_emb[:, 0], deep_walk_emb[:, 1])[0][1]), \n",
    "             2) == 0\n",
    "assert round(abs(np.corrcoef(walklets_emb[:, 0], walklets_emb[:, 1])[0][1]), \n",
    "             2) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, here we get a list of category ids to color data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id = nx.get_node_attributes(gcc_cora, 'category')\n",
    "category_id = list(category_id.values())\n",
    "category_id[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6*4))\n",
    "cases = [[laplacian_emb, 'Laplacian Eigenmaps'], \n",
    "         [svd_emb, 'Truncated SVD'], \n",
    "         [deep_walk_emb, 'DeepWalk'], \n",
    "         [walklets_emb, 'Walklets']]\n",
    "for i, (emb, title) in enumerate(cases):\n",
    "    plt.subplot(4, 1, i+1)\n",
    "    plt.scatter(emb[:, 0], emb[:, 1], c=category_id, cmap=plt.cm.Set1, s=10)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Test set size dependency (1.5 points)\n",
    "\n",
    "Let us compare the quality of embedding algorithms on classification tasks depending on the size of test set.\n",
    "\n",
    "Write a function `embeddings_score` that takes a graph and computes 16d embeddigns, splits the dataset (X is embedding, y is category id) into train and test sets, fit `GradientBoostingClassifier` and returns a list of lists:\n",
    "* Micro-F1 score of Laplacian Eigenmaps for the test size 0.99, 0.95, 0.9, 0.8, 0.7\n",
    "* The same for Truncated SVD\n",
    "* The same for Deepwalk\n",
    "* The same for Walklets\n",
    "\n",
    "*Hints:* \n",
    "* *Use `train_test_split` splitting method from sklearn*\n",
    "* *Use `f1_score(y_test, y_pred, average='micro')` method from sklearn to calculate Micro-F1 score*\n",
    "* *It is ok if it takes about 2 minutes in Colab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "caedbca1cb156918d9abf85070f2bf70",
     "grade": false,
     "grade_id": "cell-0c29359bd3423c87",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def embeddings_score(graph):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e87289002e3433f59ad35b6a31e4eb65",
     "grade": true,
     "grade_id": "cell-e2b5bae45ef6c5da",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "scores = embeddings_score(gcc_cora)\n",
    "scores = np.array(scores)\n",
    "assert scores.shape == (4, 5)\n",
    "assert scores.mean() > 0.5\n",
    "X = np.stack([np.ones(5), np.arange(5)], axis=1)\n",
    "y = scores.mean(axis=0)\n",
    "assert (np.linalg.inv(X.T @ X) @ X.T @ y)[0] > 0.4\n",
    "mean_res = scores.mean(axis=1)\n",
    "assert mean_res[0] > mean_res[3] > mean_res[1] > mean_res[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "labels = ['Laplacian Eigenmaps', 'Truncated SVD', 'DeepWalk', 'Walklets']\n",
    "for i, score in enumerate(scores):\n",
    "    plt.plot([0.99, 0.95, 0.9, 0.8, 0.7], score, label=labels[i])\n",
    "    plt.scatter([0.99, 0.95, 0.9, 0.8, 0.7], score)\n",
    "plt.legend()\n",
    "plt.title('Embedding algorithms quality')\n",
    "plt.xlabel('Test set size')\n",
    "plt.ylabel('Micro-F1 score')\n",
    "plt.gca().invert_xaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Embedding dimensionality importance (2 points)\n",
    "\n",
    "Another important property of the embedding is an ability to represent nodes in low-dimensional space. It will be great to quickly compress the most important and drop all excess imformation. Let us check how the dimensionality affects to quality and time cost.\n",
    "\n",
    "Write a function `embeddings_dim` that takes a graph, computes embeddings, splits dataset into train and test sets with test size 0.95, computes Micro-F1 scores, time costs and returns a tuple:\n",
    "* list of lists:\n",
    "  * Micro-F1 score for 8d Laplacian Eigenmaps, Truncated SVD, DeepWalk, Walklets\n",
    "  * The same for 16d\n",
    "  * The same for 32d\n",
    "  * The same for 64d\n",
    "  * The same for 128d\n",
    "* list of lists:\n",
    "  * Time cost (seconds) for 8d Laplacian Eigenmaps, Truncated SVD, DeepWalk, Walklets\n",
    "  * The same for 16d\n",
    "  * The same for 32d\n",
    "  * The same for 64d\n",
    "  * The same for 128d\n",
    "\n",
    "*Hints:*\n",
    "* *Use `time()` to get a current time moment*\n",
    "* *It is ok if it takes about 4 minutes in Colab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e53b6fcfe115a51f18327c24f7e6fd81",
     "grade": false,
     "grade_id": "cell-e039ba7855f9792c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def embeddings_dim(graph):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60b646ec695c905339864704f109f838",
     "grade": true,
     "grade_id": "cell-60b4e70331597579",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "scores, time_cost = embeddings_dim(gcc_cora)\n",
    "scores, time_cost = np.array(scores), np.array(time_cost)\n",
    "assert scores.shape == time_cost.shape == (5, 4)\n",
    "smean = scores.mean(axis=0)\n",
    "assert smean.argmin() == 2\n",
    "assert smean[1] < smean[0]\n",
    "assert smean[1] < smean[3]\n",
    "assert smean.mean() > 0.5\n",
    "tcmean = time_cost.mean(axis=0)\n",
    "assert tcmean.argmin() in [0, 1]\n",
    "assert tcmean.argmax() == 3\n",
    "assert time_cost[0, 0] < time_cost[-1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "labels = ['Laplacian Eigenmaps', 'Truncated SVD', 'DeepWalk', 'Walklets']\n",
    "for i, score in enumerate(scores.T):\n",
    "    plt.plot([8, 16, 32, 64, 128], score, label=labels[i])\n",
    "    plt.scatter([8, 16, 32, 64, 128], score)\n",
    "plt.legend()\n",
    "plt.xscale('log', basex=2)\n",
    "plt.xlabel('Dimensionality')\n",
    "plt.ylabel('Micro-F1 score')\n",
    "plt.title('Dimensionality vs score')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "for i, cost in enumerate(time_cost.T):\n",
    "    plt.plot([8, 16, 32, 64, 128], cost, label=labels[i])\n",
    "    plt.scatter([8, 16, 32, 64, 128], cost)\n",
    "plt.legend()\n",
    "plt.xscale('log', basex=2)\n",
    "plt.yscale('log', basey=2)\n",
    "plt.xlabel('Dimensionality')\n",
    "plt.ylabel('Time cost (seconds)')\n",
    "plt.title('Dimensionality vs time cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4. Node2Vec (3 points)\n",
    "\n",
    "In the current task we will define node2vec embedding.\n",
    "It is similar to the DeepWalk, but used biased random walks. It balance between breadth-  and depth-first searches to account for high order proximities.\n",
    "\n",
    "`biased_random_walk` takes graph, node for which we sample random walk, length of walk, weight p with which we will move towards previous node and weight q with which we will move away from previous node.\n",
    "\n",
    "Biased walk works with following algorithm:\n",
    "1. Start random walk from current node\n",
    "2. Find neighbors for current node in walks\n",
    "3. Assign probability weight `1 / q` for all nodes\n",
    "4. Replace probability weight with `1 / p` for  previous node  in node neighbor vector\n",
    "5. Replace probability weights for nodes represented in both previous node neighbors and current node neighbors with `1`\n",
    "6. Normalize probability weights to one.\n",
    "7. Sample next node\n",
    "Repeat sampling (2-7) while random walk size is less then `path_length`\n",
    "\n",
    "Return the array with random walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_edgelist = pd.read_csv(\"cora_cites.txt\", header=None, sep='\\t')\n",
    "\n",
    "nodes = set(cora_edgelist[0]) | set(cora_edgelist[1])\n",
    "nodeMap = dict(zip(list(nodes), range(len(nodes))))\n",
    "cora_edgelist[0] = cora_edgelist[0].map(nodeMap)\n",
    "cora_edgelist[1] = cora_edgelist[1].map(nodeMap)\n",
    "\n",
    "G = nx.from_pandas_edgelist(cora_edgelist, source=0, target=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3969a9f986a4f1865096bf05e04db42",
     "grade": false,
     "grade_id": "cell-3e6acfc34a7f8627",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def biased_random_walk(G, node, path_length, p, q):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ecfa499d7676dc5ad954280fda856af",
     "grade": true,
     "grade_id": "cell-146532f00986acf1",
     "locked": true,
     "points": 2.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "rw = biased_random_walk(G, 0, 10, 0.1, 0.5)\n",
    "\n",
    "assert len(rw) == 10\n",
    "assert rw.count(0) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biased_random_walks(G, n_walks, path_length, p, q):\n",
    "    walks = []\n",
    "    for node in G.nodes():\n",
    "        for _ in range(n_walks):\n",
    "            walk_from_node = biased_random_walk(G, node, path_length, p, q)\n",
    "            walks.append(walk_from_node)\n",
    "    return np.array(walks)\n",
    "\n",
    "def encode(walks, k):\n",
    "    walks_str = walks.astype('str').tolist()\n",
    "    model = Word2Vec(walks_str, size=k, hs=1, sg=1, alpha=0.025, iter=50, window=10)\n",
    "    embedding = np.array([model.wv[str(n)] for n in range(len(G))])\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c26cb71f58cdc7597346b77e449c829",
     "grade": true,
     "grade_id": "cell-03d420988ac76488",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "rws = biased_random_walks(G, 5, 5, 0.1, 0.1)\n",
    "embedding = encode(rws, 8)\n",
    "\n",
    "assert embedding.shape == (2708, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. GEMSEC (2 points)\n",
    "\n",
    "In this task we will realize Graph Embedding with Self Clustering (GEMSEC) model. Functions `train_gemsec` and `update_weight` is already defined. You need to realize used methods here: `initialize_embeddings`, `initialize_cluster_centers` and `step_for_pair`. You can use `random_walks` function from the previous seminars.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walks(G, n_walks, path_length):\n",
    "    walks = []\n",
    "    for node in G.nodes:\n",
    "        for i in range(n_walks):\n",
    "            walk = []\n",
    "            next_node = node\n",
    "            for j in range(path_length):\n",
    "                walk.append(next_node)\n",
    "                neighbors = [n for n in G.neighbors(next_node)]\n",
    "                next_node = np.random.choice(neighbors)\n",
    "            walks.append(walk)\n",
    "    return np.array(walks)\n",
    "\n",
    "\n",
    "def update_weight(G, source_node, target_node, embeddings, cluster_centers, gamma, lr, num_negative_samples):\n",
    "    negative_samples = sample_negative_samples(G, num_negative_samples)\n",
    "    embeddings, cluster_centers = step_for_pair(embeddings, negative_samples, source_node, target_node, cluster_centers, gamma, lr)\n",
    "    embeddings, cluster_centers = step_for_pair(embeddings, negative_samples, target_node, source_node, cluster_centers, gamma, lr)\n",
    "    return embeddings, cluster_centers\n",
    "\n",
    "def train_gemsec(G, dim, n_walks, walk_length, window_size, num_clusters, gamma, lr, num_negative_samples):\n",
    "    embeddings = initialize_embeddings(G, dim)\n",
    "    cluster_centers = initialize_cluster_centers(G, dim, num_clusters)\n",
    "    walks = random_walks(G, n_walks, walk_length)\n",
    "    for walk in tqdm(walks, total=len(walks)):\n",
    "        for i, source_node in enumerate(walk[:(walk_length - window_size)]):\n",
    "            for step in range(1, window_size + 1):\n",
    "                target_node = walk[i + step]\n",
    "                embeddings, cluster_centers = update_weight(G, source_node, target_node, embeddings, cluster_centers, gamma, lr, num_negative_samples)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`initialize_embeddings` creates np.array with normal distributed values with mean `0` and scale `1 / dimension of embedding` of size `(number of nodes in graph, dimension of embedding)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bec8ab3b7a003859f344e3e6bd6a5e2d",
     "grade": false,
     "grade_id": "cell-7884da225e939a31",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def initialize_embeddings(G, dim):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f770502eeb17176345afda10e4634319",
     "grade": true,
     "grade_id": "cell-883c28a4ccb772a0",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "embeddings = initialize_embeddings(G, 8)\n",
    "assert embeddings.shape == (2708, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster initialization works similarly, but with shape `(dimensions of embedding, number of clusters)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0897dcbc698b871c0d868518bd17d0e3",
     "grade": false,
     "grade_id": "cell-2b0050e7f1b1a149",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def initialize_cluster_centers(G, dim, num_clusters):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1cfadc64f3b5287750e23141f4fafe9",
     "grade": true,
     "grade_id": "cell-28c54d347121df3e",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "centers = initialize_cluster_centers(G, 8, 10)\n",
    "assert centers.shape == (8, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GEMSEC is trained with noise contrastive estimation. So we need to calculate negative samples for the specific node.\n",
    "We calculate negative samples as randomly chosen nodes from the graph proportionally to their degrees.\n",
    "\n",
    "You need to define function `sample_negative_samples` that takes graph and number of required negative samples as input. It returns np.array with random nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1bcc79d38b8fcb4d749377daa6e4467",
     "grade": false,
     "grade_id": "cell-a7f6f4b8ad23fdf3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sample_negative_samples(G, num_negative_samples):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8fc26c98cf88a5b2de711366820fa697",
     "grade": true,
     "grade_id": "cell-0fd6bcadeba77aaf",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "samples = sample_negative_samples(G, 5)\n",
    "assert samples.shape[0] == 5\n",
    "assert min(samples) >= min(G.nodes())\n",
    "assert max(samples) <= max(G.nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to calculate noise vector according to the sampled negatives.\n",
    "\n",
    "1. Calculate dot product between embeddings of negative samples and embedding for source node\n",
    "2. Clip calculated scores in range `(-15, 15)`\n",
    "3. Normalize scores\n",
    "4. Take a weighted mean of negative samples embeddings according to the normalized scores from 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad9831d42bdacced3f5eda07af15eb01",
     "grade": false,
     "grade_id": "cell-07da5898790aaf5e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_noise(embeddings, negative_samples, source_node):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9af2acc29eec83ea785969ce8a11dd0f",
     "grade": true,
     "grade_id": "cell-e94a43d85ed5f483",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "noise = get_noise(embeddings, samples, 0)\n",
    "assert noise.shape[0] == 8\n",
    "assert round(noise.mean(), 4) == 0.0237"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GEMSEC roots embeddings to the cluster centers. Before we try to account it, we need to find cluster corresponding to the node and the distance from its center to the node embedding.\n",
    "\n",
    "`find_cluster` function takes the embeddings array, source node, and array with cluster centers.\n",
    "\n",
    "1. Calculate difference from source embedding to the cluster centers\n",
    "2. Calculate euqlidean distance from source node embedding to the cluster centers\n",
    "3. Select the cluster as closest to the source node embedding\n",
    "4. Normalize elementwise differences from `1` by the euqlidean distance (only for found cluster)\n",
    "5. Return the index of cluster and normalized elementwise differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4892a0670848803492b230fbffa122b5",
     "grade": false,
     "grade_id": "cell-1743d123166a7920",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_cluster(embeddings, source_node, cluster_centers):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8912c3d5b45543785eddbc470d01edef",
     "grade": true,
     "grade_id": "cell-a3a5fe9d1d766113",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "idx, diff = find_cluster(embeddings, 0, centers)\n",
    "assert idx == 7\n",
    "assert round(diff.mean(), 4) == 0.2185"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient step works for two nodes: source and target node from random walk. \n",
    "\n",
    "The functions works in the following steps:\n",
    "\n",
    "1. Extract embedding for target node\n",
    "2. Calculate noise vector\n",
    "3. Find best cluster for source node and distance vector to it\n",
    "4. Calculate gradient based on following formulae\n",
    "$$grad = noise\\ vector - target\\ vector + gamma * norm\\ diff\\ from\\ cluster\\ center$$\n",
    "where gamma is clustering cost weight coefficient (hyperparam)\n",
    "5. Update source node embedding\n",
    "$$embedding(source\\ node) = embedding(source\\ node) - lr * grad$$\n",
    "where lr is learning rate (hyperparam)\n",
    "6. Update cluster center\n",
    "$$cluster\\ center\\ of\\ closest\\ cluster = cluster\\ center\\ of\\ closest\\ cluster + lr * gamma * grad$$\n",
    "7. Return embeddings and new cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "221cf54403fb397167f323d8955336c7",
     "grade": false,
     "grade_id": "cell-60e3a3b1aa35f0c0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def step_for_pair(embeddings, negative_samples, source_node, target_node, cluster_centers, gamma, lr):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c12c2cfb39e2f23412edb1b1fd00932d",
     "grade": true,
     "grade_id": "cell-2cfd8c35966e211f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "embs = train_gemsec(G, 8, 3, 3, 2, 2, 0.1, 0.1, 3)\n",
    "assert embs.shape == (2708, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
