{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment — Link Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim==3.7.0 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from tqdm.notebook import tqdm\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/netspractice/network-science/main/datasets/email-Eu-core-temporal.txt'\n",
    "open('email-Eu-core-temporal.txt', 'wb').write(requests.get(url).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Similarity based link prediction (1.5 points)\n",
    "\n",
    "Consider link prediction on the [e-mails network](http://snap.stanford.edu/data/email-Eu-core-temporal.html) where nodes are members of a research institution and edges are e-mails given with timestamps. The goal is to predict occurrence of edges in the test time period using information from the train time period only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_df = pd.read_csv(\n",
    "    'email-Eu-core-temporal.txt', \n",
    "    delimiter=' ', \n",
    "    names=['sender', 'receiver', 'timestamp']\n",
    ")\n",
    "email_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, consider the following preprocessing procedure:\n",
    "1. Select edges by given train and test time periods, for example, [0, 1000) is train and [1000, 2000) is test\n",
    "2. Build a _core_ — a network where every edge occurs at least $k_\\text{train}$ times in the train time period or at least $k_\\text{test}$ times in the test time period. Let the core be undirected, so occurrences edges (1, 0) and (0, 1) are computed together.\n",
    "3. From the core, select a train set of edges $E_\\text{train}$ that occur for the first time in the train period. All others are included to $E_\\text{test}$.\n",
    "\n",
    "Write a function `train_test_edges` that takes a pd.DataFrame `email_df` with e-mail network, a tuple with the train time period borders `train_period`, say, (0, 1000), a similar tuple `test_period`, the number of edges occurrences `ktrain` and `ktest`. The function returns two lists with tuples — train and test edges. Every edge is returned of the form where the first node is less than the second, for example [(1, 2), (2, 3)] is ok, but [(2, 1), (3, 2)] is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "045ab43c29121a4b8f0d77097b70bf4d",
     "grade": false,
     "grade_id": "cell-676bd18fcab3c342",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_test_edges(email_df, train_period, test_period, ktrain, ktest):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e939a417ca65b5dda9b203cae7f317db",
     "grade": true,
     "grade_id": "cell-a77f2fa764e87595",
     "locked": true,
     "points": 0.75,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_edges, test_edges = train_test_edges(email_df, (1e7, 2e7), (2e7, 2.5e7), 3, 3)\n",
    "_train_edges, _test_edges = np.array(train_edges), np.array(test_edges)\n",
    "assert np.all(_train_edges[:, 0] < _train_edges[:, 1])\n",
    "assert np.all(_test_edges[:, 0] < _test_edges[:, 1])\n",
    "assert len(set(train_edges).intersection(test_edges)) == 0\n",
    "assert _train_edges.shape == (4147, 2)\n",
    "assert _test_edges.shape == (418, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity based algorithm:\n",
    "1. Compute similarity matrix for all pairs of nodes except $E_\\text{train}$\n",
    "2. Order that pairs in descending of similarity\n",
    "3. Select some threshold and predict links for all pairs above the threshold\n",
    "\n",
    "Write a function `sim_link_prediction` that takes a list with train edges and test edges. The function predicts links and returns a tuple with metrics: \n",
    "* two np.arrays: FPR (false positive rate) and TPR (true positive rate) in descending of thresholds obtained by Jaccard coefficient, `nx.jaccard_coefficient`\n",
    "* the same, by Adamic/Adar index, `nx.adamic_adar_index`\n",
    "* the same, by resource allocation index, `nx.resource_allocation_index`\n",
    "\n",
    "_Hint: use `sklearn.metrics.roc_curve`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f07d3e16aa7b7b792cf03a8f0b6db6db",
     "grade": false,
     "grade_id": "cell-4125af6f7f2c4f56",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sim_link_prediction(train_edges, test_edges):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac, adam, res = sim_link_prediction(train_edges, test_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef7bd24d5cf134488c9c1cfe1ac9fabd",
     "grade": true,
     "grade_id": "cell-ea34274053a92113",
     "locked": true,
     "points": 0.75,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert jac[0].shape == jac[1].shape\n",
    "assert adam[0].shape == adam[1].shape\n",
    "assert res[0].shape == res[1].shape\n",
    "assert round(auc(jac[0], jac[1]), 4) == 0.8371\n",
    "assert round(auc(adam[0], adam[1]), 4) == 0.8500\n",
    "assert round(auc(res[0], res[1]), 4) == 0.8495"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at ROC AUC curve to compare similaritites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = [[jac[0], jac[1], 'Jaccard'], \n",
    "         [adam[0], adam[1], 'Adamic/Adar'], \n",
    "         [res[0], res[1], 'Resource alloc.']]\n",
    "for fpr, tpr, label in cases:\n",
    "    plt.plot(fpr, tpr, lw=2, \n",
    "             label='{}, AUC={:.4f}'.format(label, auc(fpr, tpr)))\n",
    "plt.plot([0, 1], [0, 1], lw=2, linestyle='--', label='Random, AUC=0.5')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. SVD node embeddings (1.5 points)\n",
    "\n",
    "Similarly to the node classification task, node embeddings could be helpful in the link prediction problem. The simplest way to obtain embeddings is to decompose some graph representation. However, in the given task, it could be helpful to factorize proximity matrices.\n",
    "\n",
    "Firstly, you need to calculate similarity matrix. `adamic_adar_similarity_matrix` function takes `train_edges` as input, calculate Adamic/Adar index between each node pairs and returns np.array with its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5a8b5449d4c6c32ffc3dc3bef1c0c17",
     "grade": false,
     "grade_id": "cell-2732118e866c35d9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def adamic_adar_similarity_matrix(train_edges):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6de0fdb23939d3175896c9a3fec31188",
     "grade": true,
     "grade_id": "cell-2b5c44d201b70101",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "adar_sim_matrix = adamic_adar_similarity_matrix(train_edges)\n",
    "assert adar_sim_matrix.shape == (1005, 1005)\n",
    "assert round(adar_sim_matrix[0, 2], 4) == 0.8523"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, graphs are sparse, so there is a high imbalance between positive (edge exists) and negative classes.\n",
    "To eliminate this problem, we can use the undersampling technique. The `negative_sampling` function should sample the unexisted edges from our graph, so they are the most similar by the number of common neighbors. The result is the list of tuples with edges (similar to the `train_edges`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b71e7d270d0c732a17bd4f2b56e35b6",
     "grade": false,
     "grade_id": "cell-1933bdf1f4862360",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def negative_sampling(train_edges, test_edges):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c87a88bdd40713374afae03ca714b8e",
     "grade": true,
     "grade_id": "cell-4fdca557d2cdfd6a",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "negatives = negative_sampling(train_edges, test_edges)\n",
    "assert len(negatives) == len(test_edges)\n",
    "assert len(set(negatives) & set(test_edges)) == 0\n",
    "\n",
    "np.random.seed(0)\n",
    "validation = np.array(negatives + test_edges)[np.random.permutation(len(negatives) * 2)]\n",
    "y_true = [int(tuple(i) in test_edges) for i in validation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you need to define `inner_product_decoder` function. It takes an array with node embeddings and a list of test edges. It should return np.array with the recovered score calculated by the dot product of embeddings for different nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b09eab904b01d3fc0dbcf731f1407930",
     "grade": false,
     "grade_id": "cell-83922c7e554f2063",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def inner_product_decoder(embeddings, test_edges):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6479051a558ebe42464a9377068686b8",
     "grade": true,
     "grade_id": "cell-78cc2ee184ce385f",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "embeddings = TruncatedSVD(n_components=8).fit_transform(adar_sim_matrix)\n",
    "\n",
    "scores = inner_product_decoder(embeddings, validation)\n",
    "tpr, fpr, _ = roc_curve(y_true, scores)\n",
    "assert round(auc(fpr, tpr), 3) == 0.843"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Edge embeddings (3 points)\n",
    "\n",
    "In the previous task, we train node level embeddings. However, for LPP, we need to have edge representation and decide whether to connect incident nodes or not.\n",
    "\n",
    "You will need to compare several techniques of edge embedding calculation from the [paper](https://peerj.com/articles/cs-172/#table-2).\n",
    "\n",
    "Compare the different vector aggregations as features for `sklearn.linear_model.LogisticRegression` with default hyperparameters.\n",
    "\n",
    "All following functions should return np.array with embeddings of edges from edges param."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average operator is simple elementwise average of node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58f462d94128ad39168d5c1651115bbc",
     "grade": false,
     "grade_id": "cell-26173c0c479136b7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def average_operator(G, embeddings, edges):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "788776768eea23698afdb7bfea3a697a",
     "grade": true,
     "grade_id": "cell-eaefe9963be3c2d0",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "G_train = nx.Graph()\n",
    "G_train.add_nodes_from(np.arange(max(set(sum(train_edges, ())) | set(sum(test_edges, ())))))\n",
    "G_train.add_edges_from(train_edges)\n",
    "\n",
    "assert round(average_operator(G_train, embeddings, validation[:1])[0, 0], 4) == 18.2539"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadamard product is an elementwise product of node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dca0930cdead56210ac1d3ed38a1da2d",
     "grade": false,
     "grade_id": "cell-f662abe3d0579575",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def hadamard_operator(G, embeddings, edges):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8d30a1af79f54290ebd174afb245efa",
     "grade": true,
     "grade_id": "cell-db04660af550adc7",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert round(hadamard_operator(G_train, embeddings, validation[:1])[0, 0], 4) == 333.1554"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted L1 is a absolute of elementwise difference between node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d6257c4c2eb4680600acc5867ddba862",
     "grade": false,
     "grade_id": "cell-5e44fbdf6a64715d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def weighted_l1_operator(G, embeddings, edges):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "077c5651b7939c80c50ea94bcd3ca8ae",
     "grade": true,
     "grade_id": "cell-f2c7878af729fcb5",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert round(weighted_l1_operator(G_train, embeddings, validation[:1])[0, 0], 4) == 0.4436"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted L2 is a square of elementwise difference between node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c81ca1cb9941e2216cb755dcb6b1d6df",
     "grade": false,
     "grade_id": "cell-0f7f5b3663337374",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def weighted_l2_operator(G, embeddings, edges):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4719541da31bf2ca18b2403fce135397",
     "grade": true,
     "grade_id": "cell-957e1faea3e9127d",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert round(weighted_l2_operator(G_train, embeddings, validation[:1])[0, 0], 4) == 0.1968"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neighbor weighted L1 is a absolute of elementwise difference between mean embeddings of node neigbors with itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91862e3967ecf48894e45a0adf8af354",
     "grade": false,
     "grade_id": "cell-9663ec4df9dbb751",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def neighbor_weighted_l1_operator(G, embeddings, edges):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0cd1f4f5e08226dded836f895956e2e4",
     "grade": true,
     "grade_id": "cell-fe7f124fa265373c",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert round(neighbor_weighted_l1_operator(G_train, embeddings, validation[:1])[0, 0], 4) == 0.2344"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neighbor weighted L1 is a square of elementwise difference between mean embeddings of node neigbors with itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6fe87475999a79b39066260f060f80ca",
     "grade": false,
     "grade_id": "cell-cb55782d381636cd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def neighbor_weighted_l2_operator(G, embeddings, edges):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11d8c3bced93a9a290125af24a502960",
     "grade": true,
     "grade_id": "cell-b3ea740a7d8e7422",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert round(neighbor_weighted_l2_operator(G_train, embeddings, validation[:1])[0, 0], 4) == 0.0549"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "operators = {\n",
    "    \"average_operator\": average_operator,\n",
    "    \"hadamard_operator\": hadamard_operator,\n",
    "    \"weighted_l1_operator\": weighted_l1_operator,\n",
    "    \"weighted_l2_operator\": weighted_l2_operator,\n",
    "    \"neighbor_weighted_l1_operator\": neighbor_weighted_l1_operator,\n",
    "    \"neighbor_weighted_l2_operator\": neighbor_weighted_l2_operator\n",
    "}\n",
    "\n",
    "train_split = int(len(validation) * 0.8)\n",
    "res = {}\n",
    "for nm, f in operators.items():\n",
    "    lr = LogisticRegression()\n",
    "    e = f(G_train, embeddings, validation)\n",
    "    lr.fit(e[:train_split], y_true[:train_split])\n",
    "    preds = lr.predict_proba(e[train_split:])[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_true[train_split:], preds)\n",
    "    res[nm] = {\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, v in res.items():\n",
    "    fpr, tpr = v['fpr'], v['tpr']\n",
    "    plt.plot(fpr, tpr, lw=2, \n",
    "             label='{}, AUC={:.4f}'.format(label, auc(fpr, tpr)))\n",
    "plt.plot([0, 1], [0, 1], lw=2, linestyle='--', label='Random, AUC=0.5')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4. Walklets (4 points)\n",
    "\n",
    "Walklets (Perozzi, Kulkarni & Skiena, 2016) use a weighted combination of embeddings of powers of adjacency matrix $A$, $A^2$, …, $A^k$ to reduce the bias of Deepwalk for low-order proximities, and approximates computing $A^i$ by skipping nodes using short random walks (Perozzi et al., 2017).\n",
    "\n",
    "The general idea is that we need to catch global graph level information for the link prediction task, not only local neighbourhood like in case with DeepWalks.\n",
    "\n",
    "Firstly, we need to sample some random walks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abd6700b9a39ce5f4f7e41bf75cb9f76",
     "grade": false,
     "grade_id": "cell-15f0d69f9b2d6d42",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# you can use this function from last task from previous seminar\n",
    "def random_walks(G, n_walks, path_length):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b01d5626527d19e6976fd5844a6ef9d",
     "grade": true,
     "grade_id": "cell-1832ce04312cc4a9",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "G = nx.karate_club_graph()\n",
    "walks = random_walks(G, 10, 5)\n",
    "\n",
    "assert walks.shape == (34*10, 5)\n",
    "for i, j in zip(walks[0, :-1], walks[0, 1:]):\n",
    "    assert G.has_edge(i, j)\n",
    "assert np.all(walks[:, 0] == np.repeat(np.arange(34), 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have random walks, we can add skips to them. Function `make_skips` separates a random walk `walk` on the several walks with steps between each `node` equal to the `length`. It returns list of lists with random walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef6cde6b4a47cdb4f400c2f946674cb0",
     "grade": false,
     "grade_id": "cell-94ff0150d47246f3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def make_skips(walk, length):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19ada594bbc3feeba1bfaa774716e64b",
     "grade": true,
     "grade_id": "cell-cf0f3db3a583d5c7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "skipped = make_skips(walks[0], 2)\n",
    "assert len(skipped) == 3\n",
    "assert len(skipped[1]) == 2\n",
    "assert skipped[1][1] == 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you need to define the function that will extract random walks with skips from the list of random walks and return another list of random walks, but with skips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f484788abab4ae524f6926f288958dcc",
     "grade": false,
     "grade_id": "cell-37b67a0e80a3f3d3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def make_skips_dataset(input_walks, length):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bae1ceb3f8a5f4740e14bb298ead5091",
     "grade": true,
     "grade_id": "cell-92b5dc686df237f5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "skipped = make_skips_dataset(walks, 2)\n",
    "assert len(skipped) == 1020\n",
    "assert len(skipped[1]) == 2\n",
    "assert skipped[1][1] == 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train embedding you need to know the set of nodes, sampled random walks without skips, size of the maximal desired skip (window_size) and dimension of embedding for the one skip.\n",
    "\n",
    "The function `train_embedding` should work as follows:\n",
    "For each skip_length between `1` and `window_size + 1`\n",
    "1. Create dataset with splits\n",
    "2. Train Word2Vec model on the created dataset with given vector_size, min_count=1, sg=1 and window=1.\n",
    "3. save embeddings for the given step\n",
    "\n",
    "After all iterations you need to take a mean of received embeddings for a node from each step. Finally, we return np.array with embeddings ordered by the id of node, if node id has no embedding, then use np.zeros(vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "479b96a3128918aa5733075c2790032a",
     "grade": false,
     "grade_id": "cell-2dd995c4351416bf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_embedding(nodes, walks, window_size=5, vector_size=8):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37a8de88912f376edf69c10838b10eb3",
     "grade": true,
     "grade_id": "cell-caa5c52f4660af25",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "G = nx.Graph(train_edges)\n",
    "nodes = np.arange(max(set(sum(train_edges, ())) | set(sum(test_edges, ()))) + 1)\n",
    "walks = random_walks(G, 10, 5)\n",
    "embeddings = train_embedding(nodes, walks)\n",
    "assert embeddings.shape == (1005, 8)\n",
    "# assert round(embeddings[0, 0], 4) == -0.1768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operators = {\n",
    "    \"average_operator\": average_operator,\n",
    "    \"hadamard_operator\": hadamard_operator,\n",
    "    \"weighted_l1_operator\": weighted_l1_operator,\n",
    "    \"weighted_l2_operator\": weighted_l2_operator,\n",
    "    \"neighbor_weighted_l1_operator\": neighbor_weighted_l1_operator,\n",
    "    \"neighbor_weighted_l2_operator\": neighbor_weighted_l2_operator\n",
    "}\n",
    "\n",
    "train_split = int(len(validation) * 0.8)\n",
    "res = {}\n",
    "for nm, f in operators.items():\n",
    "    lr = LogisticRegression()\n",
    "    e = f(G_train, embeddings, validation)\n",
    "    lr.fit(e[:train_split], y_true[:train_split])\n",
    "    preds = lr.predict_proba(e[train_split:])[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_true[train_split:], preds)\n",
    "    res[nm] = {\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, v in res.items():\n",
    "    fpr, tpr = v['fpr'], v['tpr']\n",
    "    plt.plot(fpr, tpr, lw=2, \n",
    "             label='{}, AUC={:.4f}'.format(label, auc(fpr, tpr)))\n",
    "plt.plot([0, 1], [0, 1], lw=2, linestyle='--', label='Random, AUC=0.5')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}